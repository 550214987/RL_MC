{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb2e090-e472-453c-9c0c-88cd57984260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os# Import the 'os' module for interacting with the operating system (e.g., file paths)\n",
    "import pandas as pd# Import pandas library and alias it as 'pd' for data manipulation and analysis\n",
    "import numpy as np# Import numpy library and alias it as 'np' for numerical operations\n",
    "import torch# Import PyTorch library and alias it as 'torch' for deep learning tasks\n",
    "import torch.nn as nn# Import the neural network module from PyTorch for building neural networks\n",
    "import torch.optim as optim# Import the optimization module from PyTorch for training models\n",
    "from sklearn.datasets import make_classification# Import 'make_classification' from sklearn.datasets to generate synthetic classification datasets\n",
    "from sklearn.ensemble import RandomForestClassifier# Import RandomForestClassifier from sklearn.ensemble for classification tasks using Random Forest algorithm\n",
    "from imblearn.over_sampling import SMOTE# Import SMOTE (Synthetic Minority Over-sampling Technique) from imblearn.over_sampling for handling imbalanced datasets\n",
    "from imblearn.pipeline import Pipeline# Import Pipeline from imblearn.pipeline to create a sequence of data processing steps\n",
    "from sklearn.model_selection import train_test_split# Import 'train_test_split' from sklearn.model_selection to split datasets into training and testing sets\n",
    "from sklearn.manifold import TSNE# Import TSNE (t-Distributed Stochastic Neighbor Embedding) from sklearn.manifold for dimensionality reduction and visualization\n",
    "import matplotlib.pyplot as plt# Import matplotlib.pyplot as 'plt' for creating plots and visualizations\n",
    "from sklearn.ensemble import RandomForestRegressor# Import RandomForestRegressor from sklearn.ensemble for regression tasks using Random Forest algorithm\n",
    "from sklearn.decomposition import PCA# Import PCA (Principal Component Analysis) from sklearn.decomposition for dimensionality reduction\n",
    "from sklearn.metrics import accuracy_score# Import accuracy_score from sklearn.metrics to evaluate the accuracy of classification models\n",
    "from sklearn.metrics import mean_squared_error# Import mean_squared_error from sklearn.metrics to evaluate the performance of regression models\n",
    "from scipy.stats import wasserstein_distance# Import wasserstein_distance from scipy.stats to compute the Wasserstein distance between two distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11bfad91-8d25-42f0-83fc-53e4370db688",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01794b-a225-4087-b274-f78e68bab486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Core Parameters ---------------\n",
    "TASK_TYPE = 'regression'    # Specifies the type of task: 'classification' or 'regression'\n",
    "DATA_PATH = ''        # Path to the data file\n",
    "FEATURE_COLS = ['Age','Drinking Water','Food Samples','Soil','Dust','Feces']  # List of feature column names/indices\n",
    "LABEL_COL = 'Blood Lead Levels'         # Name/index of the label column\n",
    "SAVE_DIR = ''       \n",
    "RESULT_PREFIX = 'best_result'   # Prefix for the result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703902e7-3f5b-493a-a87e-a64a05f202fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- Training Parameters ---------------\n",
    "EPISODES = 800                   # Number of training episodes (iterations)\n",
    "BATCH_SIZE = 200                # Size of each batch for training\n",
    "LEARNING_RATE = 1e-4            # Learning rate for the optimizer\n",
    "GAMMA = 0.95                    # Discount factor for future rewards (used in reinforcement learning)\n",
    "HIDDEN_DIM = 256                # Dimension of the hidden layers in the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f575292-1087-48b7-b02f-98716597dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Configuration Constraints =====================\n",
    "# Constraint Definitions\n",
    "DATA_CONSTRAINTS = {\n",
    "    # Expandable constraints for other columns, for example:\n",
    "    0: (0, 8, 'int'),      # Column 0: Value range from 0 to 8, data type 'int'\n",
    "    1: (0, 1, 'float'),    # Column 1: Value range from 0 to 1, data type 'float'\n",
    "    2: (0, 1, 'float'),    # Column 2: Value range from 0 to 1, data type 'float'\n",
    "    3: (0, 1, 'float'),    # Column 3: Value range from 0 to 1, data type 'float'\n",
    "    4: (0, 1, 'float'),    # Column 4: Value range from 0 to 1, data type 'float'\n",
    "    5: (0, 1, 'float')}    # Column 5: Value range from 0 to 1, data type 'float'\n",
    "}\n",
    "LABEL_CONSTRAINTS = {\n",
    "    'min': 0,               # Minimum value for the label\n",
    "    'max': None,            # Maximum value for the label (None means no upper limit)\n",
    "    'dtype': 'float'        # Data type of the label\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0e78b-cb4f-477f-9f6f-089a2df5eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Data Loading Module =====================\n",
    "def load_data():\n",
    "    \"\"\"Safely load data and validate integrity\"\"\"\n",
    "    try:\n",
    "        # Check file existence\n",
    "        if not os.path.exists(DATA_PATH):\n",
    "            raise FileNotFoundError(f\"Data file does not exist: {DATA_PATH}\")  \n",
    "        # Read CSV file into DataFrame\n",
    "        df = pd.read_csv(DATA_PATH)\n",
    "        print(\"Data loaded successfully, columns:\", df.columns.tolist())\n",
    "        # Process feature columns\n",
    "        if isinstance(FEATURE_COLS[0], str):\n",
    "            # If FEATURE_COLS contains column names (strings)\n",
    "            missing = [col for col in FEATURE_COLS if col not in df.columns]\n",
    "            if missing:\n",
    "                raise KeyError(f\"Missing feature columns: {missing}\")\n",
    "            # Extract feature values and convert to float32\n",
    "            X = df[FEATURE_COLS].values.astype(np.float32)\n",
    "            feature_names = FEATURE_COLS  # Save feature column names\n",
    "        else:\n",
    "            # If FEATURE_COLS contains column indices (integers)\n",
    "            feature_names = df.columns[FEATURE_COLS].tolist()  # Get column names from indices\n",
    "            X = df.iloc[:, FEATURE_COLS].values.astype(np.float32)\n",
    "        # Process label column\n",
    "        if isinstance(LABEL_COL, str):\n",
    "            # If LABEL_COL is a column name (string)\n",
    "            if LABEL_COL not in df.columns:\n",
    "                raise KeyError(f\"Label column does not exist: {LABEL_COL}\")\n",
    "            # Extract label values\n",
    "            y = df[LABEL_COL].values\n",
    "        else:\n",
    "            # If LABEL_COL is a column index (integer)\n",
    "            y = df.iloc[:, LABEL_COL].values\n",
    "        # Reshape y to 1D array\n",
    "        y = y.reshape(-1)\n",
    "        # Print data shapes\n",
    "        print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "        return X, y, feature_names  # Return features, labels and feature names\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions during data loading\n",
    "        print(f\"\\n!! Data loading error: {str(e)} !!\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1760c1b-f047-40e2-b1e4-7fed37848ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Reinforcement Learning Environment =====================\n",
    "class RLEnvironment:\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"Initialize the RL environment with training data\"\"\"\n",
    "        self.X_orig = X  # Original feature matrix\n",
    "        self.y_orig = y  # Original target values\n",
    "        # Split data into training and validation sets (80/20 split)\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            X, y, test_size=0.2#, random_state=rrrre\n",
    "        )\n",
    "        # Initialize the model based on task type\n",
    "        self.model = self._init_model()\n",
    "        # Train baseline model and evaluate its performance\n",
    "        self.baseline = self._train_baseline()\n",
    "        # Initialize adaptive penalty coefficient\n",
    "        self.adaptive_coef = 0.5  \n",
    "        # Current phase identifier ('explore', 'exploit', or 'converge')\n",
    "        self.phase = 'explore'    \n",
    "        # Current episode counter\n",
    "        self.episode = 0\n",
    "        # History of rewards received\n",
    "        self.reward_history = []\n",
    "    def _init_model(self):\n",
    "        \"\"\"Initialize model based on task type\"\"\"\n",
    "        return RandomForestClassifier(n_estimators=50) if TASK_TYPE == 'classification' \\\n",
    "               else RandomForestRegressor(n_estimators=50)\n",
    "    def _train_baseline(self):\n",
    "        \"\"\"Train and evaluate baseline model\"\"\"\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        return self._evaluate()\n",
    "    def _evaluate(self):\n",
    "        \"\"\"Evaluate model performance on validation set\"\"\"\n",
    "        pred = self.model.predict(self.X_val)\n",
    "        if TASK_TYPE == 'classification':\n",
    "            return accuracy_score(self.y_val, pred)\n",
    "        return -mean_squared_error(self.y_val, pred)  # Negative MSE for maximization\n",
    "    def calculate_reward(self, perf_gain, w_dist):\n",
    "        \"\"\"Dynamically calculate reward based on performance and constraints\"\"\"\n",
    "        # Phase-based coefficient adjustment\n",
    "        if self.phase == 'explore':\n",
    "            # Coefficient increases linearly from 0.3 to 0.5 during exploration\n",
    "            coef = max(0.3, 0.5 - 0.02 * self.episode)  \n",
    "        elif self.phase == 'exploit':\n",
    "            # Increase penalty when performance decreases during exploitation\n",
    "            coef = 0.5 + 0.1 * (perf_gain < 0)  \n",
    "        else: \n",
    "            # Strict constraints during convergence phase\n",
    "            coef = 0.7  \n",
    "        # Key feature penalty (example: first 3 features)\n",
    "        key_feature_penalty = 1.5 * np.mean(w_dist[:3])  \n",
    "        # Total penalty combines general and key feature penalties\n",
    "        total_penalty = coef * (0.7 * np.mean(w_dist) + 0.3 * key_feature_penalty)\n",
    "        return perf_gain - total_penalty\n",
    "    def evaluate_enhancement(self, X_new, y_new):\n",
    "        \"\"\"Evaluate the enhancement effect of new data\"\"\"\n",
    "        # Combine original and new data\n",
    "        X_combined = np.vstack([self.X_train, X_new])\n",
    "        y_combined = np.concatenate([self.y_train, y_new])\n",
    "        # Retrain model on combined dataset\n",
    "        self.model.fit(X_combined, y_combined)\n",
    "        # Evaluate performance after enhancement\n",
    "        current_score = self._evaluate()\n",
    "        perf_gain = current_score - self.baseline\n",
    "        # Calculate Wasserstein distance for each feature\n",
    "        w_dist_per_feature = [wasserstein_distance(self.X_train[:,i], X_new[:,i]) \n",
    "                              for i in range(self.X_train.shape[1])]\n",
    "        # Calculate final reward considering both performance and constraints\n",
    "        reward = self.calculate_reward(perf_gain, w_dist_per_feature)\n",
    "        # Phase transition logic based on episode count and performance\n",
    "        self.episode += 1\n",
    "        self.reward_history.append(reward)\n",
    "        # Transition to exploit phase if consistently performing well\n",
    "        if self.episode > 100 and np.mean(self.reward_history[-10:]) > 0:\n",
    "            self.phase = 'exploit' \n",
    "        # Transition to converge phase if feature distributions stabilize\n",
    "        elif self.episode > 200 and np.mean(w_dist_per_feature) < 0.1:\n",
    "            self.phase = 'converge'\n",
    "        return reward, np.mean(w_dist_per_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b77605-7429-47fd-ba09-6b8828828818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Policy Network =====================\n",
    "class AugmentPolicy(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"Initialize the policy network\"\"\"\n",
    "        super().__init__()\n",
    "        # Neural network architecture:\n",
    "        # - Input layer to hidden layer with ReLU activation\n",
    "        # - Hidden layer to output layer (2 parameters: alpha and beta)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, HIDDEN_DIM),  # Input to hidden layer\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Linear(HIDDEN_DIM, 2)  # Output alpha and beta parameters\n",
    "        )\n",
    "        # Learnable log standard deviation parameter\n",
    "        # Initialized to zeros, will be exponentiated during forward pass\n",
    "        self.log_std = nn.Parameter(torch.zeros(2))\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of the policy network\"\"\"\n",
    "        # Compute mean parameters through the network\n",
    "        means = self.net(x)\n",
    "        # Compute standard deviation from log_std parameter\n",
    "        # Apply exponential and clamp to avoid numerical instability\n",
    "        std = torch.exp(self.log_std).clamp(min=1e-6)\n",
    "        return means, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7378e70-eff7-4069-ae61-b8880ccc012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Reinforcement Learning Agent =====================\n",
    "class AugmentAgent:\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"Initialize the RL agent with policy network and optimizer\"\"\"\n",
    "        # Policy network that generates augmentation parameters\n",
    "        self.policy = AugmentPolicy(input_dim)\n",
    "        # Adam optimizer for policy network parameters\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=LEARNING_RATE)\n",
    "    def generate_actions(self, states):\n",
    "        \"\"\"Generate augmentation parameters (alpha and beta) for given states\"\"\"\n",
    "        # Convert input states to PyTorch tensor\n",
    "        states = torch.FloatTensor(states)\n",
    "        # Disable gradient calculation during action generation\n",
    "        with torch.no_grad():\n",
    "            # Get mean and standard deviation from policy network\n",
    "            means, stds = self.policy(states)\n",
    "            # Create normal distribution with learned parameters\n",
    "            dist = torch.distributions.Normal(means, stds)\n",
    "            # Sample augmentation parameters from the distribution\n",
    "            actions = dist.sample()\n",
    "        # Convert actions to numpy array for further processing\n",
    "        return actions.numpy()\n",
    "    def update_policy(self, states, actions, rewards):\n",
    "        \"\"\"Update policy network using policy gradient method\"\"\"\n",
    "        # Convert all inputs to PyTorch tensors\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        # Calculate discounted returns (Monte Carlo returns)\n",
    "        discounted_returns = []\n",
    "        G = 0.0  # Initialize return\n",
    "        for r in reversed(rewards.numpy()):\n",
    "            G = r + GAMMA * G  # G_t = r_t + γ * G_{t+1}\n",
    "            discounted_returns.insert(0, G)  # Insert at beginning to maintain order\n",
    "        # Normalize returns to reduce variance in policy gradient\n",
    "        returns_tensor = torch.FloatTensor(discounted_returns)\n",
    "        returns_norm = (returns_tensor - returns_tensor.mean()) / (returns_tensor.std() + 1e-6)\n",
    "        # Compute policy gradient loss\n",
    "        means, stds = self.policy(states)\n",
    "        dist = torch.distributions.Normal(means, stds)\n",
    "        # Calculate log probabilities of taken actions\n",
    "        log_probs = dist.log_prob(actions).sum(dim=1)\n",
    "        # Policy gradient loss: -E[logπ(a|s) * G]\n",
    "        loss = -(log_probs * returns_norm).mean()\n",
    "        # Perform policy optimization step\n",
    "        self.optimizer.zero_grad()  # Clear previous gradients\n",
    "        loss.backward()  # Backpropagate loss\n",
    "        nn.utils.clip_grad_norm_(self.policy.parameters(), 1.0)  # Gradient clipping\n",
    "        self.optimizer.step()  # Update parameters\n",
    "        # Note: The clipping helps prevent exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a555cd-0207-453f-95b6-fab2deff1999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Data Range Validation Module =====================\n",
    "class DataValidator:\n",
    "    @staticmethod\n",
    "    def apply_constraints(data, constraints):\n",
    "        \"\"\"\n",
    "        Apply constraint conditions to generated data\n",
    "        Parameters:\n",
    "            data: Input data matrix to be processed (numpy.ndarray)\n",
    "            constraints: Dictionary format {column_index: (min_val, max_val, dtype)}\n",
    "        \"\"\"\n",
    "        # Create a copy of the input data to avoid modifying the original\n",
    "        constrained_data = data.copy()\n",
    "        # Iterate through each column constraint\n",
    "        for col_idx, (min_val, max_val, dtype) in constraints.items():\n",
    "            # Apply range limitation to the specified column\n",
    "            constrained_data[:, col_idx] = np.clip(\n",
    "                constrained_data[:, col_idx], \n",
    "                min_val, \n",
    "                max_val\n",
    "            )\n",
    "\n",
    "            # Apply data type conversion based on the specified dtype\n",
    "            if dtype == 'int':\n",
    "                # Round to nearest integer and convert to integer type\n",
    "                constrained_data[:, col_idx] = np.round(\n",
    "                    constrained_data[:, col_idx]\n",
    "                ).astype(int)\n",
    "            elif dtype == 'float':\n",
    "                # Convert to floating point type\n",
    "                constrained_data[:, col_idx] = constrained_data[:, col_idx].astype(float)\n",
    "        \n",
    "        # Return the data after applying all constraints\n",
    "        return constrained_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337727f-4aab-412d-a89d-5825cb577d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Data Generator =====================\n",
    "class DataGenerator:\n",
    "    @staticmethod\n",
    "    def generate_batch(X, y, actions, feature_constraints=None, label_constraints=None):\n",
    "        \"\"\"Generate augmented data batch (fixed version)\"\"\"\n",
    "        # Transform action parameters to appropriate ranges\n",
    "        alphas = (actions[:,0] + 1) / 2   # Map from [-1,1] to [0,1]\n",
    "        betas = (actions[:,1] + 1) * 0.1  # Map from [-1,1] to [0,0.2]\n",
    "        X_new = []  # Container for generated feature samples\n",
    "        y_new = []  # Container for generated label samples\n",
    "        # Pre-generate all sample indices for efficiency\n",
    "        indices = np.random.choice(len(X), (len(actions), 2), replace=True)\n",
    "        for i in range(len(actions)):\n",
    "            # Get transformed action parameters for current sample\n",
    "            alpha = alphas[i]\n",
    "            beta = betas[i]\n",
    "            # Randomly select two sample indices for interpolation\n",
    "            idx1, idx2 = indices[i]\n",
    "            # ================= Feature Generation =================\n",
    "            # Linear interpolation between two samples with added noise\n",
    "            sample = alpha * X[idx1] + (1-alpha) * X[idx2] + beta * np.random.randn(*X[0].shape)\n",
    "            # Apply feature constraints if specified\n",
    "            if feature_constraints:\n",
    "                # Reshape for constraint application and then flatten back\n",
    "                sample = DataValidator.apply_constraints(\n",
    "                    sample.reshape(1,-1), \n",
    "                    feature_constraints\n",
    "                ).flatten()\n",
    "            X_new.append(sample)\n",
    "            # ================= Label Generation =================\n",
    "            if TASK_TYPE == 'classification':\n",
    "                # For classification: randomly choose one of the two labels\n",
    "                label = y[idx1] if np.random.rand() > 0.5 else y[idx2]\n",
    "            else:\n",
    "                # For regression: weighted average of the two labels\n",
    "                label = alpha * y[idx1] + (1-alpha) * y[idx2]\n",
    "                # Apply label constraints if specified\n",
    "                if label_constraints:\n",
    "                    # Clip label to specified min/max range\n",
    "                    label = np.clip(label, \n",
    "                                  label_constraints.get('min', -np.inf),\n",
    "                                  label_constraints.get('max', np.inf))\n",
    "                    # Type conversion based on specified dtype\n",
    "                    dtype = label_constraints.get('dtype', 'float')\n",
    "                    if dtype == 'int':\n",
    "                        label = int(round(label))\n",
    "                    elif dtype == 'float':\n",
    "                        label = float(label)\n",
    "            y_new.append(label)\n",
    "        # Convert lists to numpy arrays before returning\n",
    "        return np.array(X_new), np.array(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f15d9-0279-4e37-b266-8b0d049bc0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Visualization Module =====================\n",
    "class ResultVisualizer:\n",
    "    @staticmethod\n",
    "    def plot_pca_comparison(original, augmented, save_path):\n",
    "        \"\"\"PCA projection comparison visualization\"\"\"\n",
    "        # Initialize PCA with 2 components for 2D visualization\n",
    "        pca = PCA(n_components=2) \n",
    "        # Combine original and augmented data for joint projection\n",
    "        combined = np.vstack([original, augmented])\n",
    "        # Fit PCA and transform the combined data\n",
    "        projected = pca.fit_transform(combined)\n",
    "        # Create visualization figure\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        # Plot original data points in 2D PCA space\n",
    "        plt.scatter(\n",
    "            projected[:len(original),0], \n",
    "            projected[:len(original),1], \n",
    "            alpha=0.3, \n",
    "            label='Original data'  # Original data\n",
    "        )\n",
    "        # Plot augmented data points in 2D PCA space\n",
    "        plt.scatter(\n",
    "            projected[len(original):,0], \n",
    "            projected[len(original):,1], \n",
    "            alpha=0.3, \n",
    "            label='Augmented data'  # Augmented data\n",
    "        )\n",
    "        # Set title with explained variance ratio\n",
    "        plt.title(\n",
    "            f\"PCA projection comparison (Explained variance ratio: {pca.explained_variance_ratio_.sum():.2f})\"\n",
    "            # PCA projection comparison (Explained variance ratio)\n",
    "        )\n",
    "        # Add legend to distinguish data types\n",
    "        plt.legend()\n",
    "        # Save the visualization to file\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    @staticmethod\n",
    "    def save_training_history(history, save_path):\n",
    "        \"\"\"Training history visualization with split curves\"\"\"\n",
    "        # Create a wider figure for better visualization\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        # ----------------- Reward Curve Subplot -----------------\n",
    "        plt.subplot(1, 2, 1)  # Left subplot position (1 row, 2 columns, first plot)\n",
    "        # Plot reward history with custom styling\n",
    "        plt.plot(\n",
    "            history['episode'], \n",
    "            history['reward'], \n",
    "            color='#2e7d32',  # Green color\n",
    "            linewidth=2, \n",
    "            label='Combined reward'  # Combined reward\n",
    "        )\n",
    "        # Set title and labels for reward subplot\n",
    "        plt.title('Reward trend', fontsize=14, pad=20)  # Reward trend\n",
    "        plt.xlabel('Training episodes', fontsize=12)  # Training episodes\n",
    "        plt.ylabel('Reward value', fontsize=12)  # Reward value\n",
    "        plt.grid(True, alpha=0.3)  # Add grid with transparency\n",
    "        plt.legend()  # Add legend\n",
    "        # ----------------- Wasserstein Distance Curve Subplot -----------------\n",
    "        plt.subplot(1, 2, 2)  # Right subplot position (second plot)\n",
    "        # Plot W-distance history with custom styling\n",
    "        plt.plot(\n",
    "            history['episode'], \n",
    "            history['w_dist'], \n",
    "            color='#c62828',  # Red color\n",
    "            linewidth=2, \n",
    "            label='Wasserstein distance'  # Wasserstein distance\n",
    "        ) \n",
    "        # Set title and labels for W-distance subplot\n",
    "        plt.title('Data distribution distance trend', fontsize=14, pad=20)  # Data distribution distance trend\n",
    "        plt.xlabel('Training episodes', fontsize=12)  # Training episodes\n",
    "        plt.ylabel('W-distance', fontsize=12)  # W-distance\n",
    "        plt.grid(True, alpha=0.3)  # Add grid with transparency\n",
    "        plt.legend()  # Add legend\n",
    "        # Adjust subplot spacing and save the figure\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69e7cb-5b06-4b16-aa1e-b590a994695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Enhanced Data Saving Module =====================\n",
    "class DataSaver:\n",
    "    @staticmethod\n",
    "    def save_enhanced_data(original_data, original_labels, enhanced_data, enhanced_labels, \n",
    "                          feature_names, label_name, save_dir, prefix):\n",
    "        \"\"\"Save both enhanced data and combined dataset with proper metadata\n",
    "        Added enhanced_labels parameter\"\"\"\n",
    "        # Create save directory if it doesn't exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # Generate timestamp for unique filenames\n",
    "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_name = f\"{prefix}_{timestamp}\"\n",
    "        # ================= Save Enhanced Data =================\n",
    "        # Create DataFrame for enhanced data with original feature names\n",
    "        enhanced_df = pd.DataFrame(enhanced_data, columns=feature_names)\n",
    "        # Add labels to the DataFrame using original label column name\n",
    "        enhanced_df[label_name] = enhanced_labels\n",
    "        # Save enhanced data as CSV with original column names\n",
    "        enhanced_df.to_csv(os.path.join(save_dir, f\"{base_name}_enhanced.csv\"), index=False)\n",
    "        # Also save as numpy array for potential later use\n",
    "        np.save(os.path.join(save_dir, f\"{base_name}_enhanced.npy\"), enhanced_data)\n",
    "        # ================= Save Combined Dataset =================\n",
    "        # Combine original and enhanced data\n",
    "        combined_data = np.vstack([original_data, enhanced_data])\n",
    "        combined_labels = np.concatenate([original_labels, enhanced_labels])\n",
    "        # Create DataFrame for combined dataset\n",
    "        combined_df = pd.DataFrame(combined_data, columns=feature_names)\n",
    "        # Add labels to the combined DataFrame\n",
    "        combined_df[label_name] = combined_labels\n",
    "        # Save combined dataset as CSV\n",
    "        combined_df.to_csv(os.path.join(save_dir, f\"{base_name}_full.csv\"), index=False)\n",
    "        # ================= Save Metadata =================\n",
    "        # Calculate Wasserstein distance between original and enhanced features\n",
    "        w_dist = np.mean([\n",
    "            wasserstein_distance(original_data[:,i], enhanced_data[:,i]) \n",
    "            for i in range(original_data.shape[1])\n",
    "        ])\n",
    "        # Create metadata dictionary\n",
    "        meta = {\n",
    "            'original_samples': original_data.shape[0],  # Number of original samples\n",
    "            'enhanced_samples': enhanced_data.shape[0],   # Number of enhanced samples\n",
    "            'w_distance': w_dist,                         # Average Wasserstein distance\n",
    "            'generation_time': timestamp                  # Generation timestamp\n",
    "        }\n",
    "        # Save metadata as CSV\n",
    "        pd.Series(meta).to_csv(os.path.join(save_dir, f\"{base_name}_meta.csv\"))\n",
    "def plot_adaptive_coef(history):\n",
    "    \"\"\"Plot the dynamic penalty coefficient change trend\"\"\"\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(\n",
    "        history['episode'], \n",
    "        history['coef'], \n",
    "        color='#2e7d32',  # Green color\n",
    "        linewidth=2\n",
    "    )\n",
    "    plt.title('Dynamic penalty coefficient trend')  # Dynamic penalty coefficient trend\n",
    "    plt.xlabel('Training episodes')  # Training episodes\n",
    "    plt.ylabel('Penalty coefficient')  # Penalty coefficient\n",
    "    plt.grid(True, alpha=0.3)  # Add grid with transparency\n",
    "    # Save plot to specified directory\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'dynamic_coef.png'), dpi=300)\n",
    "    plt.close()  # Close the figure to free memory\n",
    "def log_phase_transition(phase_log):\n",
    "    \"\"\"Visualize the distribution of training phases\"\"\"\n",
    "    phases = ['explore', 'exploit', 'converge']\n",
    "    phase_counts = [phase_log.count(p) for p in phases]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.pie(\n",
    "        phase_counts, \n",
    "        labels=phases, \n",
    "        autopct='%1.1f%%',\n",
    "        colors=['#ff9999','#66b3ff','#99ff99']  # Custom colors for each phase\n",
    "    )\n",
    "    plt.title('Training phase distribution ratio')  # Training phase distribution ratio\n",
    "    # Save pie chart to specified directory\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'phase_dist.png'), dpi=300)\n",
    "    plt.close()  # Close the figure to free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d2470-1fa3-48d1-ab3b-1a31429fc50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================== Main Function =====================\n",
    "def main():\n",
    "    \"\"\"Main training loop for the data augmentation RL system\"\"\" \n",
    "    # Initialize environment and directories\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    # Load data and get feature names\n",
    "    X, y, feature_names = load_data()  # Get feature column names\n",
    "    # Initialize RL environment and agent\n",
    "    env = RLEnvironment(X, y)\n",
    "    agent = AugmentAgent(X.shape[1])\n",
    "    # Initialize training history recorder\n",
    "    history = {\n",
    "        'episode': [],\n",
    "        'reward': [],\n",
    "        'w_dist': [],\n",
    "        'alpha_mean': [],\n",
    "        'alpha_std': [],\n",
    "        'beta_mean': [],\n",
    "        'beta_std': [],\n",
    "        'coef': [],\n",
    "        'phase': []\n",
    "    }\n",
    "    # Initialize best result tracker\n",
    "    best = {'w_dist': float('inf'), 'samples': None, 'model': None}\n",
    "    # Initialize phase transition log\n",
    "    phase_log = []\n",
    "    # Training loop\n",
    "    for ep in range(EPISODES):\n",
    "        # Generate action parameters for current batch\n",
    "        states = np.repeat([X.mean(axis=0)], BATCH_SIZE, axis=0)\n",
    "        actions = agent.generate_actions(states)\n",
    "        # Generate augmented data with constraints\n",
    "        X_new, y_new = DataGenerator.generate_batch(\n",
    "            env.X_train, \n",
    "            env.y_train, \n",
    "            actions,\n",
    "            feature_constraints=DATA_CONSTRAINTS,  # Apply feature constraints\n",
    "            label_constraints=LABEL_CONSTRAINTS  # Apply label constraints\n",
    "        )\n",
    "        # Evaluate augmentation effect\n",
    "        reward, w_dist = env.evaluate_enhancement(X_new, y_new)\n",
    "        # Update policy network\n",
    "        agent.update_policy(states, actions, [reward]*BATCH_SIZE)\n",
    "        # Record training metrics\n",
    "        history['episode'].append(ep+1)\n",
    "        history['reward'].append(float(reward))\n",
    "        history['w_dist'].append(float(w_dist))\n",
    "        # Record action parameter statistics\n",
    "        alphas = (actions[:,0] + 1) / 2\n",
    "        betas = (actions[:,1] + 1) * 0.2\n",
    "        history['alpha_mean'].append(float(np.mean(alphas)))\n",
    "        history['alpha_std'].append(float(np.std(alphas)))\n",
    "        history['beta_mean'].append(float(np.mean(betas)))\n",
    "        history['beta_std'].append(float(np.std(betas)))\n",
    "        # Record dynamic penalty coefficient and current phase\n",
    "        if env.phase == 'explore':\n",
    "            coef = max(0.3, 0.5 - 0.02 * env.episode)\n",
    "        elif env.phase == 'exploit':\n",
    "            coef = 0.5 + 0.1 * (reward < 0)\n",
    "        else: \n",
    "            coef = 0.7\n",
    "        history['coef'].append(coef)\n",
    "        history['phase'].append(env.phase)\n",
    "        phase_log.append(env.phase)\n",
    "        # Update best results if current performance is better\n",
    "        if w_dist < best['w_dist']:\n",
    "            best['w_dist'] = w_dist\n",
    "            best['samples'] = X_new\n",
    "            best['model'] = agent.policy.state_dict()\n",
    "            best.update(w_dist=w_dist, samples=X_new, y_new=y_new, model=agent.policy.state_dict())\n",
    "            # Visualize PCA comparison when finding better solution\n",
    "            ResultVisualizer.plot_pca_comparison(\n",
    "                env.X_orig, \n",
    "                X_new, \n",
    "                os.path.join(SAVE_DIR, f'{RESULT_PREFIX}_pca.png')\n",
    "            )\n",
    "        # Print training progress\n",
    "        print(f\"Episode {ep+1}/{EPISODES}, Reward: {reward:.2f}, W距离: {w_dist:.4f}, Phase: {env.phase}\")\n",
    "    # Save final results\n",
    "    print(best['w_dist'])\n",
    "    pd.DataFrame(history).to_csv(\n",
    "        os.path.join(SAVE_DIR, f'{RESULT_PREFIX}_history.csv'), \n",
    "        index=False\n",
    "    )\n",
    "    torch.save(\n",
    "        best['model'], \n",
    "        os.path.join(SAVE_DIR, f'{RESULT_PREFIX}_model.pth')\n",
    "    )\n",
    "    np.savez(\n",
    "        os.path.join(SAVE_DIR, f'{RESULT_PREFIX}_samples.npz'), \n",
    "        X=best['samples'], \n",
    "        y=y_new[:len(best['samples'])]\n",
    "    )\n",
    "    # Save training history visualization\n",
    "    ResultVisualizer.save_training_history(\n",
    "        history, \n",
    "        os.path.join(SAVE_DIR, f'{RESULT_PREFIX}_training.png')\n",
    "    )\n",
    "    # Save best augmented data with all required parameters\n",
    "    if best['samples'] is not None:\n",
    "        DataSaver.save_enhanced_data(\n",
    "            original_data=env.X_orig,\n",
    "            original_labels=env.y_orig,\n",
    "            enhanced_data=best['samples'],\n",
    "            enhanced_labels=best['y_new'],\n",
    "            feature_names=feature_names,        # Pass feature column names\n",
    "            label_name=LABEL_COL,                # Pass label column name\n",
    "            save_dir=SAVE_DIR,\n",
    "            prefix=RESULT_PREFIX\n",
    "        )\n",
    "    # Generate final visualizations\n",
    "    plot_adaptive_coef(history)\n",
    "    log_phase_transition(phase_log)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
